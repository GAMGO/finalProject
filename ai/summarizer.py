import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
EMBED_MODEL = "klue/bert-base"

class ReviewSummarizer:
    def __init__(self, model_name: str = EMBED_MODEL):
        print("ðŸ”„ Loading Extractive Summarizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name).to(DEVICE)
        self.model.eval()
        print("âœ… Summarizer Loaded")

    def sentence_split(self, text: str):
        # ë‹¨ìˆœ ë¬¸ìž¥ ë¶„ë¦¬
        s = text.replace("?", ".").replace("!", ".")
        return [x.strip() for x in s.split(".") if len(x.strip()) > 0]

    def embed(self, sentence: str):
        tokens = self.tokenizer(
            sentence,
            return_tensors="pt",
            truncation=True,
            max_length=128
        )
        tokens = {k: v.to(DEVICE) for k,v in tokens.items()}

        with torch.no_grad():
            output = self.model(**tokens)
        cls = output.last_hidden_state[:, 0]
        return cls.cpu().numpy()[0]

    def summarize(self, text: str, max_sentences: int = 2):
        if not text or len(text.strip()) == 0:
            return ""

        sents = self.sentence_split(text)

        # ë¬¸ìž¥ í•˜ë‚˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(sents) <= max_sentences:
            return text

        # ë¬¸ìž¥ ìž„ë² ë”©
        embeddings = np.vstack([self.embed(s) for s in sents])

        # ì¤‘ìš”ë„ ì ìˆ˜ (L2 norm)
        scores = np.linalg.norm(embeddings, axis=1)

        # ìƒìœ„ ë¬¸ìž¥ ì„ íƒ
        idxs = scores.argsort()[::-1][:max_sentences]
        idxs = sorted(idxs)  # ì›ëž˜ ìˆœì„œ ìœ ì§€

        summary = ". ".join([sents[i] for i in idxs])
        return summary.strip()